# ğŸš™ Offroad Segmentation ONNX C++ ğŸŒ²

## ğŸ” Overview
This project demonstrates the complete lifecycle of a semantic segmentation model for offroad environments - from dataset preparation to training to production deployment. Its primary goal is to benchmark Python vs C++ inference performance using ONNX as the interchange format.

## ğŸ¯ Key Features
- Complete ML pipeline demonstration (dataset â†’ training â†’ production)
- Performance benchmarking between Python and C++ inference
- Reproducible environment using Docker ğŸ³
- ONNX model format for cross-platform compatibility
- Semantic segmentation for offroad navigation

## ğŸ—ï¸ Project Structure
```
â”œâ”€â”€ dataset/                # Dataset handling code
â”‚   â””â”€â”€ goose-dataset/      # GOOSE dataset for offroad segmentation
â”œâ”€â”€ docker/                 # Docker configuration
â”œâ”€â”€ model/                  # Model architecture definitions
â”œâ”€â”€ onnx_inference/         # C++ inference implementation
â”‚   â””â”€â”€ src/                # C++ source code
â””â”€â”€ wandb/                  # Weights & Biases experiment tracking
```

## ğŸ“Š GOOSE Dataset
This project utilizes the GOOSE (Ground Offroad Outdoor SEgmentation) dataset, which provides labeled images for offroad environments. The dataset includes:
- RGB and NIR (Near Infrared) image pairs
- Semantic segmentation masks for traversability analysis
- Various offroad scenes and conditions

## ğŸ› ï¸ Getting Started
```bash
# Clone the repository
git clone https://github.com/yourusername/offroad-segmentation-onnx-cpp.git
cd offroad-segmentation-onnx-cpp

# Run the docker container
docker-compose up -d

# Download the dataset
./dataset/get_dataset.sh
```

## ğŸ§  Model Training
```bash
# Train the segmentation model
python train_goose.py
```

## ğŸš€ Inference

### Python Inference
```python
# Import the model from the factory
from model.factory import create_model

# Load an image and run inference
# (See train_goose.py for complete example)
```

### C++ Inference
```bash
# Build the C++ inference engine
cd onnx_inference
mkdir build && cd build
cmake ..
make

# Run inference on an image
./segmentation_inference <image_path>
```

## ğŸ“ˆ Benchmarking
This project allows direct comparison between Python and C++ inference performance:
- Throughput (FPS)
- Latency (ms)
- Memory usage
- CPU/GPU utilization

## ğŸ³ Docker Environment
All dependencies and configurations are encapsulated in Docker, ensuring consistent and reproducible results across different machines.

```bash
# Build and start the Docker environment
docker-compose up -d
```

## ğŸ“ License
See the LICENSE file for details.